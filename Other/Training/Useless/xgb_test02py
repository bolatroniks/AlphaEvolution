#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Jan  5 23:37:54 2017

@author: renato
"""

#Import libraries:
import time
import pandas as pd
import numpy as np
import xgboost as xgb
from xgboost.sklearn import XGBClassifier
from sklearn import cross_validation, metrics   #Additional scklearn functions
from sklearn.grid_search import GridSearchCV   #Perforing grid search

import matplotlib.pylab as plt

from matplotlib.pylab import rcParams

from Trading.Dataset.Dataset import Dataset

from sklearn.grid_search import GridSearchCV, RandomizedSearchCV

rcParams['figure.figsize'] = 12, 4
global_c = 1
global_c2 = 1

param_dist = {"max_depth": [4,5, 6,7,8,9,None],
    "max_features": [8,9,10,11,16,22],
    "min_samples_split": [8,10,11,14,16,19],
    "min_samples_leaf": [1,2,3,4,5,6,7],
    "bootstrap": [True, False]}

#here we specify the search settings, we use only 25 random parameter 
#valuations but we manage to keep training times in check.

def loadDataSetForXGB (series_no=1):
    pass

def custom_obj(preds, dtrain):    
    labels = dtrain.get_label().astype(int)
    
    aux_labels = np.zeros((len(labels),3))
    
    for j in range(len(aux_labels)):
        aux_labels[j,labels[j]] = 1
    #labels = aux
    
    
    #preds[:,0] += preds[:,1] / global_c
    #preds[:,2] += preds[:,1] / global_c
    #preds_aux = np.zeros(np.shape(preds))
    #preds_aux[:,0] = preds[:,0] / (preds[:,0] + preds[:,1] + preds[:,2])
    #preds_aux[:,1] = preds[:,1] / (preds[:,0] + preds[:,1] + preds[:,2])
    #preds_aux[:,2] = preds[:,2] / (preds[:,0] + preds[:,1] + preds[:,2])
    
    preds = 1.0 / (1.0 + np.exp(-preds))
    grad_aux = np.zeros (np.shape(preds))
    hess_aux = np.zeros (np.shape(preds))
    
    for i in range(len (grad_aux)):
        if labels [i] == 0:
            grad_aux [i,0] = (preds[i,0]+preds[i,1]*global_c) - 1
            grad_aux [i,1] = ((preds[i,1]) - 0) * global_c2
            grad_aux [i,2] = (preds[i,2] - 0)
        elif labels [i] == 2:
            grad_aux [i,2] = (preds[i,2]+preds[i,1]*global_c) - 1
            grad_aux [i,1] = ((preds[i,1]) - 0) * global_c2
            grad_aux [i,0] = (preds[i,0] - 0)
        else:
            grad_aux [i,0] = (preds[i,0] - 0)
            grad_aux [i,2] = (preds[i,2] - 0)
            grad_aux [i,1] = (preds[i,1] - 1)
            
    
    #grad_aux [labels==0,:] = preds[labels==0,:] - aux_labels[labels==0,:]
    hess_aux = preds * (1.0-preds)
    
    #interleaves grad and hess
    grad = np.zeros(3*len(grad_aux))
    hess = np.zeros(3*len(hess_aux))
    for i in range(len (grad_aux)):
        for k in range (3):
            grad [i*3 + k] = grad_aux [i, k]
            hess [i*3 + k] = hess_aux [i, k]
    
    return grad, hess

def getSetsFromDataset (ds, bConvolveCdl=False, mm_200_idx=8, bNormalize_by_mm200=True, bShuffleTrainset = True):
    X = ds.X[:,-1,:]
    y = np.dot(ds.y, [0,1,2])
    cv_X = ds.cv_X[:,-1,:]
    cv_y = np.dot(ds.cv_y, [0,1,2])
    test_X = ds.test_X[:,-1,:]
    test_y = np.dot(ds.test_y, [0,1,2])
    
    if bConvolveCdl == True:
        exp_fn = np.exp(-np.linspace(0,40,20)/4)
        for i in range (81,142):
            #print i
            X[:,i] = np.convolve (X[:,i], exp_fn)[:len(X)]
            cv_X[:,i] = np.convolve (cv_X[:,i], exp_fn)[:len(cv_X)]
            test_X[:,i] = np.convolve (test_X[:,i], exp_fn)[:len(test_X)]
    
    if bNormalize_by_mm200 == True:
        for i in ds.mu_sigma_list:
            X[:,i] = X[:,i] / X[:,mm_200_idx]
            cv_X[:,i] = cv_X[:,i] / cv_X[:,mm_200_idx]
            test_X[:,i] = test_X[:,i] / test_X[:,mm_200_idx]

    #shuffle train set
    if bShuffleTrainset == True:
        idx = np.linspace(0,len(X)-1,len(X), dtype=int)
        np.random.shuffle(idx)
        X = X[idx, :]
        y = y[idx]

    return X, y, cv_X, cv_y, test_X, test_y

def model_train (X, y, param=None, num_round=5, bExpWeight=True):
    if param is None:
        param = {}
        param['objective'] = 'multi:softprob'
        param['eval_metric'] = 'auc'
        param['max_depth'] = 5
        param['eta'] = 0.01
        param['silent'] = 0
        param['updater'] = 'grow_gpu'
        param['num_class'] = 3
        param['min_child_weight'] = 5
    
    if bExpWeight == True:
        a_w = np.exp(np.linspace(0,len(X)-1, len(X))/len(X))
        a_w /= np.mean(a_w)
    else:
        a_w = np.ones(len(X))

    dtrain = xgb.DMatrix(X, label=y, weight=a_w)
    #dtest = xgb.DMatrix(cv_X, label=cv_y)
    #watchlist = [(dtest, 'eval'), (dtrain, 'train')]
    tmp = time.time ()
    
    bst = xgb.train(param, dtrain, num_round, obj=custom_obj)
    print ('Elapsed time in training: '+str(time.time () - tmp))
    return bst
    
def evaluate_model (bst, cv_X, cv_y, min_threshold=0.5, bPrintCharts=True):
    preds = bst.predict(xgb.DMatrix(cv_X))
    
    
    preds [:,0] = preds [:,0] > min_threshold
    preds [:,2] = preds [:,2] > min_threshold
    
    predictions = np.argmax(preds, axis=1)
    acc = metrics.accuracy_score(cv_y[predictions!=1], predictions[predictions!=1])    
    
    neutral = len(cv_y[predictions==1])
    
    
    if bPrintCharts == True:
        print "Accuracy : %.4g" % acc
        print ('Neutral predictions:'+str(neutral))
        plt.figure ()
        plt.hist (preds[:,0], bins=10, label='Short predictions')
        plt.legend(loc='best')
        plt.show ()
            
        feat_imp = pd.Series(bst.get_fscore()).sort_values(ascending=False)
        feat_imp[0:20].plot(kind='bar', title='Feature Importances')
        plt.ylabel('Feature Importance Score')
    return acc, neutral

def modelfit(alg, X, y, cv_X, cv_y, useTrainCV=False, cv_folds=5, early_stopping_rounds=50):
    
    if useTrainCV:
        xgb_param = alg.get_xgb_params()
        xg_cv = xgb.DMatrix(cv_X, label=cv_y)
        cvresult = xgb.cv(xgb_param, xg_cv, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,
            metrics='auc', early_stopping_rounds=early_stopping_rounds)
        alg.set_params(n_estimators=cvresult.shape[0])
    
    #Fit the algorithm on the data
    alg.fit(X, y,eval_metric='auc')
        
    #Predict cv set:
    predictions = alg.predict(cv_X)
    predprob = alg.predict_proba(cv_X)[:,1]
        
    #Print model report:
    print "\nFeatures Report"
    print "Accuracy : %.4g" % metrics.accuracy_score(cv_y[predictions!=1], predictions[predictions!=1])
    #print "AUC Score (Train): %f" % metrics.roc_auc_score(cv_y, predprob)
                    
    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)
    feat_imp.plot(kind='bar', title='Feature Importances')
    plt.ylabel('Feature Importance Score')
    
    plt.figure ()
    plt.plot(predictions[0:150], label='Predictions')
    plt.plot(cv_y[0:150], label='cv_labels')
    plt.legend(loc='best')
    plt.show()
    

series_list=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,40,41,42,43,44,46,47,48,49,50,
             51,52,53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 
             72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 98, 99,
             100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116]

series_list2 = [301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315]

if True:
    ds = Dataset(lookback_window=2, n_features=142)
    #ds.featpath = './datasets/Fx/Featured/NotNormalizedNoVolume/NewFeatures'
    #ds.feat_filename_prefix = 'not_normalized_new_feat_'
    ds.featpath = './datasets/Fx/Featured/NotNormalizedNoVolume'
    
    param = {}
    param['objective'] = 'multi:softprob'
    param['eval_metric'] = 'auc'
    param['max_depth'] = 8
    param['eta'] = 0.1
    param['subsample'] = 0.75
    param['colsample_bytree'] = 0.85
    param['silent'] = 0
    param['updater'] = 'grow_gpu'
    param['num_class'] = 3
    param['min_child_weight'] = 10

    global_c = 0.5
    global_c2 = 0.5    
    num_trees = 20
    
    series_no = 1
    
    for test_set_size in [2000,1750,1500,1250,1000,750,500,250]:
        ds.period_ahead = 1
        ds.last=1500
        ds.cv_set_size = 250
        ds.test_set_size = test_set_size    
        ds.loadSeriesByNo (series_no, bRelabel=False, bNormalize=True, bConvolveCdl=True)
        plt.figure ()
        plt.plot(ds.cv_X[:,-1,0])
        plt.show ()
    
        a, b, eur_cv_X, eur_cv_y, eur_test_X, eur_test_y = getSetsFromDataset (ds, bShuffleTrainset=True)
        bst = model_train (a,b, param, num_round=num_trees)
        acc, neutral = evaluate_model (bst, eur_cv_X, eur_cv_y, 
                                               min_threshold=0.79, bPrintCharts=True)


if False:
    for series_no in [1,2,3,4,5,6,7,8,9,10,11,12]:
        ds = Dataset(lookback_window=2, n_features=142)
        #ds.featpath = './datasets/Fx/Featured/NotNormalizedNoVolume/NewFeatures'
        #ds.feat_filename_prefix = 'not_normalized_new_feat_'
        ds.featpath = './datasets/Fx/Featured/NotNormalizedNoVolume'
        
        ds.period_ahead = 1
        ds.last=2000
        
        ds.loadSeriesByNo (series_no, bRelabel=False, bNormalize=True, bConvolveCdl=True)
        
        param = {}
        param['objective'] = 'multi:softprob'
        param['eval_metric'] = 'auc'
        param['max_depth'] = 8
        param['eta'] = 0.1
        param['subsample'] = 0.75
        param['colsample_bytree'] = 0.75
        param['silent'] = 0
        param['updater'] = 'grow_gpu'
        param['num_class'] = 3
        param['min_child_weight'] = 10
    
        global_c = 0.5
        global_c2 = 0.5    
        num_trees = 50
        a, b, eur_cv_X, eur_cv_y, eur_test_X, eur_test_y = getSetsFromDataset (ds, bShuffleTrainset=True)    
        
        #ds.loadDataSet(series_list=series_list, end=9, bRelabel=False, bNormalize=True)
        #ds.createSingleTrainSet (y_width=3)
        #rX, ry, a, b, c, d = getSetsFromDataset(ds, bShuffleTrainset=True)
        
        
        bst = model_train (a,b, param, num_round=num_trees)
        acc, neutral = evaluate_model (bst, eur_cv_X, eur_cv_y, 
                                               min_threshold=0.59, bPrintCharts=True)
        bst = model_train (eur_cv_X,eur_cv_y, param, num_round=num_trees)
        acc, neutral = evaluate_model (bst, eur_test_X, eur_test_y, 
                                               min_threshold=0.59, bPrintCharts=True)
#Choose all predictors except target & IDcols
#predictors = [x for x in train.columns if x not in [target, IDcol]]
if False:
    ds = Dataset(lookback_window=2, n_features=142)
    ds.featpath = './datasets/Fx/Featured/NotNormalizedNoVolume'
    
    ds.period_ahead = 1
    ds.last=2000
    #ds.mu_sigma_list=[0,1,2,3,6,7,8,9,24,25,26,35,37,47,48,49,50,51,52,57,59,60,73,80]
    
    ds.loadSeriesByNo (1, bRelabel=False, bNormalize=False)
    a, b, eur_cv_X, eur_cv_y, eur_test_X, eur_test_y = getSetsFromDataset (ds)
    ds.loadSeriesByNo (3, bRelabel=False, bNormalize=False)
    a, b, yen_cv_X, yen_cv_y, yen_test_X, yen_test_y = getSetsFromDataset (ds)
    ds.loadSeriesByNo (2, bRelabel=False, bNormalize=False)
    a, b, zar_cv_X, zar_cv_y, zar_test_X, zar_test_y = getSetsFromDataset (ds)
    
    #ds.loadSeriesByNo(1, bRelabel=False, bNormalize=False)
    ds.loadDataSet(series_list=series_list, end=50, bRelabel=False, bNormalize=False)
    ds.createSingleTrainSet (y_width=3)
    
    X, y, a, b, c, d = getSetsFromDataset (ds)
    rX, ry, a, b, c, d = getSetsFromDataset(ds, bShuffleTrainset=True)
    del a
    del b
    del c
    del d
    import gc
    gc.collect ()
    
    max_depth_list = [5,6,7,8]
    num_round_list=[250,500,1000,1500,2000,2500]
    eta_list=[0.25,0.1,0.05,0.025,0.01]
    min_child_w_list=[5,10,15,20]
    c_list = [1,0.75,0.5,0.25,0.1]
    c2_list = [1,0.75,0.5,0.25,0.1]
    subsample_list = [0.5,0.6,0.7,0.8,0.9,1.0]
    colsample_bytree_list = [0.5,0.6,0.7,0.8,0.9,1.0]
    eval_list=['auc']
   
    
    n_iter = 1
    for i in range (n_iter):
        f = open ('./models/performance/xgb_random_search_02.txt', 'a')     
        param = {}
        param['objective'] = 'multi:softprob'
        param['eval_metric'] = eval_list [np.int(np.random.uniform(0,len(eval_list)))]
        param['max_depth'] = max_depth_list [np.int(np.random.uniform(0,len(max_depth_list)))]        
        param['eta'] = eta_list[np.int(np.random.uniform(0,len(eta_list)))]
        param['subsample'] = subsample_list[np.int(np.random.uniform(0,len(subsample_list)))]
        param['colsample_bytree'] =colsample_bytree_list[np.int(np.random.uniform(0,len(colsample_bytree_list)))]
        param['silent'] = 0
        param['updater'] = 'grow_gpu'
        param['num_class'] = 3
        param['min_child_weight'] = min_child_w_list [np.int(np.random.uniform(0,len(min_child_w_list)))]        
    
        global_c = c_list [np.int(np.random.uniform(0,len(c_list)))]        
        global_c2 = c2_list [np.int(np.random.uniform(0,len(c2_list)))]        
        num_trees = num_round_list [np.int(np.random.uniform(0,len(num_round_list)))]        

        bst = model_train (rX,ry, param, num_round=num_trees)
        
        f.write ('Iteration: '+str(i)+', ')
        f.write ('c: '+str(global_c)+', ')
        f.write ('c2: '+str(global_c2)+', ')
        f.write ('#trees: '+str(num_trees)+', ')
        f.write (str(param)+', ')
        
        min_thres_list = [0.49,0.59, 0.69, 0.79, 0.89]
        for threshold in min_thres_list:
            acc, neutral = evaluate_model (bst, eur_cv_X, eur_cv_y, 
                                           min_threshold=threshold, bPrintCharts=False)
            
            f.write ('EUR - Threshold: '+str(threshold)+', Accuracy: '+str(acc)+', Neutral: '+str(neutral)+', ')
            
            acc, neutral = evaluate_model (bst, yen_cv_X, yen_cv_y, 
                                           min_threshold=threshold, bPrintCharts=False)
            
            f.write ('Yen - Threshold: '+str(threshold)+', Accuracy: '+str(acc)+', Neutral: '+str(neutral)+', ')
            
            acc, neutral = evaluate_model (bst, zar_cv_X, zar_cv_y, 
                                           min_threshold=threshold, bPrintCharts=False)
            
            f.write ('ZAR - Threshold: '+str(threshold)+', Accuracy: '+str(acc)+', Neutral: '+str(neutral)+', ')
        f.write ('\n')
        f.close ()  
    #bst_shuffled_100trees = model_tran (rX, ry, param, num_round=100)
    #evaluate_model (bst_shuffled_100trees, cv_X, cv_y)
    #bst_shuffled_200trees = model_train (rX, ry, param, num_round=200)
    #evaluate_model (bst_shuffled_200trees, cv_X, cv_y)
    
    
    
    if False:    
        xgb4 = XGBClassifier(
         learning_rate =0.01,
         n_estimators=500,
         max_depth=8,
         min_child_weight=20,
         gamma=0,
         subsample=0.8,
         colsample_bytree=0.8,
         reg_alpha=0.005,
         updater='grow_gpu',
         objective= 'multi:softprob',
         nthread=4,
         scale_pos_weight=1,
         seed=27)
        
        
        modelfit(xgb4, X[0:, :], y[0:], cv_X, cv_y)

if False:
    #test sknn
    from sknn.mlp import Layer,Classifier
    from sknn import *
    
    nn = mlp.Classifier(
                layers=[
                    mlp.Layer("Tanh", units=256),
                    mlp.Layer("Sigmoid", units=128),
                    mlp.Layer("Softmax", units=3)],
                n_iter=5000,
                n_stable=100,
                batch_size=128,
                learning_rate=0.002,
                learning_rule="momentum",
                valid_size=0.1,
                verbose=1)
    nn.fit(X, y)    
    nn.score(cv_X, cv_y)
    predictions = np.reshape(nn.predict(cv_X), len(cv_X))
    print "Accuracy : %.4g" % metrics.accuracy_score(cv_y[predictions!=1], predictions[predictions!=1])
    
    
    #test with keras
    model = Sequential()
    #model.add(Dense(256))
    #model.add(Dense(256, input_dim=(None,142)))
    model.add(Dense(256, input_dim=(142)))
    model.add(Activation('tanh'))
    model.add(Dense(128))
    model.add(Activation('sigmoid'))
    model.add(Dense(3))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
    
    model.fit (X, ds.y, batch_size=128)
    predictions = np.argmax(model.predict(cv_X), axis=1)
    metrics.accuracy_score(cv_y, predictions)

if False:
    from Trading.Training.TradingModel import *
    
    modelname = 'trained_model_normalized_feat_stateless_fixed_timesteps_more_complex_512_2x_more_dropout'
    modelpath = './models/weights'
    
    my_model = TradingModel(modelname=modelname, modelpath=modelpath)
    my_model.dataset.n_features = 142
    my_model.dataset.lookback_window = 126
    my_model.buildModel ()
    my_model.dataset.featpath = './datasets/Fx/Featured/NotNormalizedNoVolume'
    
    my_model.dataset.loadDataSet(series_list=series_list, end=10, bRelabel=False)
    my_model.dataset.createSingleTrainSet (y_width=3)

    my_model.dataset.X = normalizeOnTheFly(my_model.dataset.X)
    my_model.model.fit(my_model.dataset.X, my_model.dataset.y, batch_size=128, nb_epoch=10)
    
    my_model.dataset.cv_X = normalizeOnTheFly(my_model.dataset.cv_X)
    predictions = np.argmax(my_model.model.predict(my_model.dataset.cv_X), axis=1)
    print "Accuracy : %.4g" % metrics.accuracy_score(np.dot(my_model.dataset.cv_y, [0,1,2])[predictions!=1], predictions[predictions!=1])

    
    